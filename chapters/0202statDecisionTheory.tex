\chapter{Statistical decision theory}
  
  \section{Definition of statistical decision theory}
    % TODO Add ref https://www.britannica.com/science/decision-theory-statistics
    \textbf{Statistical decision theory} is a set of quantitative methods for reaching optimal decisions for well posed problems. Statistical decision theory applies to supervised learning, while it does not apply to unsupervised learning. For most of the course we will focus on supervised learning. 

  \section{Supervised learning}
    \textbf{Supervised learning} is a set of methods whose objective is, given the observations $(\vec{x}_i, y_i)$ with $i = 1, \dots, n$, to find a rule $\hat{f}$ that allows us to predict $Y$ from $\vec{X}$, meaning that $\hat{Y} = \hat{f}(\vec{X})$. $\hat{f}(\vec{X})$ is thus an approximation of the true rule $f(\vec{X})$. Finding $\hat{f}$ corresponds to training (or fitting) a model using labelled data pairs (both predictors and response values are known).

    Broadly, there are two main types of methods of supervised learning: 
    \begin{itemize}
      \item \textbf{Regression methods}, in which the response $Y$ is quantitative (numerical). In this case $\hat{f}$ is called \textbf{regression function}.
      \item \textbf{Classification methods}, in which the response $Y$ is qualitative (categorical). In this case $\hat{f}$ is called \textbf{classifier}.
    \end{itemize}
    
    The predictors ($\vec{X}$) can take any form, numerical or categorical. In general there no assumptions on the form of the predictors (but not always).
    
    To evaluate a model you use \textbf{loss functions}, which are functions used to penalize the differences between $Y$ and $f(\vec{X})$. Many loss functions can be used; the choice of a specific loss function determines which function is considered to be the true rule $f(\vec{X})$.

  \section{Regression setting}
    In a regression setting, the loss function which is generally used is the \textbf{squared error loss function}, which is defined as
    $$L(Y,f(\vec{X})) = (Y-f(\vec{X}))^2$$
    When using this loss function, the criterion for choosing the model becomes finding $f$ such that it minimizes the \textbf{expected prediction error} (EPE), which is the expected value of the squared error loss function:
    \begin{align*}
      \text{EPE}(f) & = E_{Y,\vec{X}}[(Y-f(\vec{X}))^2] \\
                    & = \iint (y - f(\vec{x}))^2 g_{Y,\vec{X}}(y, \vec{x})\,dy\,d\vec{x} & \text{if } Y \text{ is continuous}
    \end{align*}
    This error is an expectation since you usually do not know the distribution of $Y$ and $\vec{X}$.
    If Y is continuous you can rewrite this expectation as the double integral times the joint density function $g$.
    
    \begin{theorem}[Minimum of the expected prediction error]
      $\text{EPE}(f) = E_{Y,\vec{X}}[(Y-f(\vec{X}))^2]$ has a \textbf{minimum} when 
      $f(\vec{x}) = E[Y|\vec{X} = \vec{x}]$.
      $f(\vec{x})$ is called regression function.
    \end{theorem}
    % TODO Maybe it is needed to add pedices to the expectations

    \begin{proof}
      (Sketch)
      By definition we know that
      $$\text{EPE}(f) = E_{Y,\vec{X}}[(Y-f(\vec{X}))^2]$$

      Then, since the law of iterated expectations states that $E[X] = E[E[X|Y]]$ (analogously to profile likelihood, you fix one variable and variate the other), we get 

      $$E_{Y,\vec{X}}[(Y-f(\vec{X}))^2] = E_{\vec{X}}[E_{Y|\vec{X}}[(Y-f(\vec{X}))^2]| \vec{X}]$$
      
      If we then consider a specific value of $\vec{X}$, meaning $\vec{X} = \vec{x}$, the inner expectation becomes 
      
      $$E[(Y-f(\vec{x}))^2 | \vec{X} = \vec{x}\,] $$
      
      But since $\vec{x}$ is fixed, $f(\vec{x})$ is a constant, hence this expectation can be written as a function in some parameter $a$

      $$g(a) = E_{Y}[(Y-a)^2]$$

      Then we want to find the (constant) value of $a$ that minimizes $g(a)$
      \begin{align*}
        g(a) & = E_{Y}[(Y-a)^2]               & \\
             & = E[Y^2 - 2aY + a^2]           & \text{compute the square} \\
             & = E[Y^2] -2aE[Y] + E[a^2]      & \text{separate expectations} \\
             & = E[Y^2] -2aE[Y] + a^2         & \text{pull out } a^2 \text{ since constant} \\
      \end{align*}

      Then, setting the derivative to zero we get 

      $$\frac{dg}{da} = -2E[Y] + 2a \mbeq 0 \implies \hat{a} = E[Y]$$

      Notice that if you plug $\hat{a}$ into $g(a) = E_{Y}[(Y-a)^2]$ you get

      $$g(\hat{a}) = E_{Y}[(Y-E[Y])^2] = Var(Y)$$
      
      Since $a = E[Y]$, the function that minimizes the EPE is 

      $$f(\vec{x}) = E[Y|\vec{X} = \vec{x}]$$
    \end{proof}
    % TODO Some steps could be clearer

    In a regression setting you could use other loss functions, 
    This is not the only choice
    $$L(Y|f(\vec{X})) = |Y - f(\vec{X})|$$
    $f(\vec{x}) = \text{ median of } Y \text{ given } \vec{x}$ 
    LAD regression (least absolute deviation)
    Robust statistics (median is more robust to outliers)

    We will choose the squared error loss and this motivates how the methods are developed.
    For example:
    Linear regression: $$Y|\vec{X} = \vec{x} \sim N(\vec{x}^t\vec{\beta}, \sigma^2)$$
    Way to express linear regression focussing on conditional mean
    $$E[Y|\vec{X} = \vec{x}] = \beta_0 + \beta_1x_1 + \dots + \beta_px_p$$ (parametric)

    K nearest neighbour (non-parametric approach for regression)

    K nearest neighbour 
    % TODO add graph 
    $$\hat{f}(\vec{x}) = \text{ Average } (y_i | \vec{x}_i \in N_K(\vec{x}))$$
    Sample mean, neigbourhood of $k$ points closest to $\vec{x}$

    It makes two approximations:
    Using sample mean to approximate population mean
    Uses a neighbourhood of $\vec{x}$ rather than only $\vec{x}$

    Works well if a lot of data and p (number of predictors is small)

    This simple approach inspired the development of more sophisticated kernel methods.

  \section{Classification setting}
    Response $Y$ is categorical, with $K$ categories. 
    A classifier is deciding which class to assign to a new observation $\vec{x}$.
    Our $\hat{Y}$ in this case is the predicted class.

    $0-1$ LOSS The loss function basically penalizes choosing wrong
    $$ L(Y, \hat{Y}(\vec{X})) =$$
    True class predicted class
    % TODO True false table 
    % TODO k*k mat
    In this case every mistake counts as 1

    Leading to the criterion: find $\hat{y}$ that minimizes
    Expected 0-1 loss 
    $$ E[L(Y, \hat{Y}(\vec{X}))]$$
    Main question: what function of $\vec{X}$ minimizes the expected 0-1 loss

    Let us concentrate on one $\vec{x}$: (inner part of the expectation)
    $$E[L(Y, \hat{Y}(\vec{X}))] = \sum_{k=1}^k L(k, \hat{Y}(\vec{x}))p(k|\vec{x})$$
    Considering the binary case, if the classifier predicts $\vec{x}$ to class 0 ($\hat{y} = 0$),

    \begin{align*}
      E[L(Y,0)] & = L(0,0)p(0|\vec{x}) + L(1, 0)p(1|\vec{x}) \\
                & = 0 \cdot p(0|\vec{x}) + 1 \cdot p(1|\vec{x}) \\
                & = p(1|\vec{x})
    \end{align*}

    If the classifier predicts $\vec{x}$ to class 1 ($\hat{y} = 1$),
    \begin{align*}
      E[L(Y,1)] & = L(0,1)p(0|\vec{x}) + L(1, 1)p(1|\vec{x}) \\
                & = 1 \cdot p(0|\vec{x}) + 0 \cdot p(1|\vec{x}) \\
                & = p(0|\vec{x})
    \end{align*}

    Predicting $\vec{x}$ to the class that minimizes the expected loss results in:
    classifying $\vec{x}$ to class 1 if $$p(1|\vec{x}) > p(0|\vec{x})$$
    Since $p(0|\vec{x}) = 1 - p(1|\vec{x})$, we have that
    $$p(1|\vec{x}) > 1 - p(1|\vec{x}) \implies 2p(1|\vec{x}) > 1 \implies p(1|\vec{x})>0.5$$ 

    In general ($k$ classes), the 0-1 loss is minimized by the following rule:
    Bayes classifier
    % TODO Fix formula?
    Assign $\vec{x}$ to class 
    $$j = \argmax_{i\,\in\,classes} p(Y = j|\vec{X} = \vec{x})$$

    (assign x to the class with the highest probability)
    Just tells us how to set the problem, but then the single method has to make us estimate the probabilities.
    Bayes classifier is optimal in 0-1 loss

    Try with k = 3 and classes 0,1,2

    In the binary case:
    $$
    \text{assign } \vec{x} \text{ to class} 
    \begin{cases}
      1 \text{ if } p(1|\vec{x}) > 0.5\\
      0 \text{ if } p(1|\vec{x}) < 0.5
    \end{cases}
    $$
    
    % TODO add decision boundary graph
    $p(1|\vec{x}) = 0.5$ Bayes decision boundary

    Bayes error rate 
    % TODO Make brackets bigger
    $$\text{BER } = 1 - E_{\vec{X}}(\max_{j} p(j|\vec{x}))$$
    Probability of committing an error 

    % TODO Add graphs
    $$\text{perfect separation } \implies \max_{j} p(j|\vec{x}) = 1 \implies \text{ BER } = 0$$

    $$\text{BER } > 0$$ (irreducible error)

    There is a more general loss function:
    Misclassification loss (binary example)
    $$L(Y, \hat{Y}(\vec{x})) = $$ % TODO Add table
    Misclassification errors are not equal 
    $C_0$ is the misclassification cost of misclassifying a class 0 to 1
    $C_1$ is the misclassification cost of misclassifying a class 1 to 0
    Often the errors are not equal (credit risk, medical context)
    Often there are unbalanced classes (rare disease, but if you do not penalize enough "Diseased" you get a classifier which has high accuracy but just tells that everyone in healty)

    If we repeat the same procedure with this 
    $$E[L(Y,0)] = c_1 p(1|\vec{x})$$
    $$E[L(Y,1)] = c_0 p(0|\vec{x})$$
    So assign $\vec{x}$ to the class 1 if 
    $$c_1 p(1|\vec{x}) > c_0 p(0|\vec{x})$$
    Since $$p(0|\vec{x}) = 1 - p(1|\vec{x})$$
    $$c_1 p(1|\vec{x}) > c_0 - c_0 p(1|\vec{x}))$$
    $$p(1|\vec{x}) > \frac{c_0}{c_0 + c_1}$$
    New threshold that takes into account different errors.

  \section{Model accuracy}
    In practice, we would use our chosen method to estimate $f(\vec{x})=E[Y|\vec{X}=\vec{x}]$ in a regressionm or $p(1|\vec{x})$ in a classification from training data $(\vec{x}_i, y_i), \, i = 1, \dots, n$

    Accuracy is typically measured on test data 
    $$ (\vec{x}_i^{(t)}, y_i^{(t)}), \, i = 1, \dots, m $$
    in a regression setting, using MSE
    % TODO Fix brackets
    $$\text{MSE } = \frac{1}{m} \sum_{i = 1}^{m}(y_i^{(t)} - \hat{f}(\vec{x}_i^{(t)}))^2$$

    In classification estimate and plug in formula
    Confusion matrix 
    % TODO Add table true negative and so on

    $$\text{TPR } = \frac{\text{TP}}{\text{TP } + \text{ TN}}$$ sensitivity 
    $$\text{FPR } = \frac{\text{FP}}{\text{TN } + \text{ FP}}$$
    $$ = 1 - \frac{\text{TN}}{\text{TN } + \text{ FP}}$$ 1 - specificity
    $$\text{ERROR RATE } = \frac{\text{FP } + \text{ FN}}{m}$$ 
    These above only work if you assume a 1-0 loss, no if different

    If costs are unequal, you still count misclassifications but multiply by weight
    $$Miscalssification cost = \frac{c_0 \text{ FP} + c_1 \text{ FN}}{m}$$
    Since precise costs are difficult to say, better to plot Receiver Operating Characteristic (ROC) curve
    Basically you change all possible values of the threshold from zero to one; extrema will be wrong but some value in the middle would be fine, thus you remake the confusion matrix with the new cutoff
    FPR vs TPR for all possible thresholds $t \in [0, 1]$
    % TODO Add ROC graph 
    Best possible case would be top left corner (deterministic) so you want the highest curve you can
    Diagonal worst case (would be the same as tossing a coin)
    Area under the curve is called AUC (the higher the better, best 0.5, worst 0)
    If lines cross you decide based on which area you are interested in 
    
    
  